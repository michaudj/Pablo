# -*- coding: utf-8 -*-
"""
Created on Mon Aug 26 20:35:33 2019

@author: ajd
"""

from math import exp
import numpy as np
import matplotlib.pyplot as plt
from random import seed,random,randint
import copy
import json
seed()
# =============================================================================
# Some typing terms:
#primitive: one symbol
#compund: type contains / or \ and thus more than one primitive
#simple primitive: all existing compound types that reduce to primitive only contains primitive. n/n   s\s/s  a\a    etc.
#complex primitive: at least one existing compuond type that reduces to primitive contains at least two symbols. existence of a\b makes b a complex primitive etc.
# =============================================================================
#
#   MOST IMPORTANT CHOICES MADE IN CURRENT PILOT
#
#1. Two untyped elements don't get types unless a succesful identification of a sentence occurs.
#2. First typing occurs att positive reinforcement of identified sentence in which no elements have types. Whatever is identified as a sentence then gets all types that reduce to s using only one primitive. For example if two elements, s/s, s and s, s\s. One is chosen randomly and reinforced.
#3. If one element gets typed as s and the other element does not have (good) types, the other element is not typed, because being an s means "goal is reached". Other primitives do assign new types to the other element because we know that they need to be chunked with something to reach the goal. i.e. if s1 is a, s2 is typed as a first choice a\s and if s2 is a s1 is typed as a first choice s/a. there is of course the possibility of s1 and s2 being s/s and s\s respectively, but my choice here is to cut when something is a sentence, as a first choice.
#4. If a sentence is successfully identified and 1 element is typed and the other not, the untyped element (if it is not chunked, for simplicity chunks are not typed) should be typed as s over or under the other element, even if the typed element is s or if this results in the assignment of a negative type. this is the only case in which a negative type gets a second chance!
#5. Types with a values below zero are never assigned to element. They are maintained as a type of that element to prevent that this type is invented. This can lead to emergence of new primitives.
#6. All probabilistic behaviuour decisions are made following the same principle: exp(beta*b_value) and then a probabilistic choice between the modified b-values. this is also applied when choosing among types. As soon as any v-value is involved this applies.
#7. When choosing a compatible pair for two elements that have several types, the following procedure is applied: 1. Choose one of the types for both elements probabilistically. 2. If there are compatible types for the other element, choose one of them probabilistically. 3. if not, the first chosen type is excluded from the scramble and the procedure restarts. 4. If no compatible pair is found, the first chosen type will assign a new type to the other element. An alternative to this procedure is to identify all compatible pairs and then choose one probabilistically. If there are many types, this will be computationally costly due to many compatibility checks, and it seems less cognitively plausible than current alternative.
#8. For simplicity, chunked typings are not retyped. Also, untyped chunks are not typed. The only exception is 2 and 4 above. In non-post-border situations, s1 is never retyped or typed before decision-making. This can be changed in future versions.
#9. Type support and support from substantial stimuli to behaviour are equally weighted, i.e. they are considered equally important. Expectation is types will take over without any favoring bias in model, bc types are more frequent.
#10. WRONG: Type learner replaces subchunk learner. In the current version type learner is only compatible with right-chunking learner. Otherwise two subchunking systems will run parallelly, seems suboptimal. One problem is that the "substantial side" will not have acces to substrings. for example, there can be an ungrammatical formulaic expression embedded in a sentence, and the system will then have to recognize the whole sentence. In future versions, the substantial side should have access to substrings. These substrings should probably be generated by typings, again, to avoid two parallell subchunking systems.
#11. Decision support from types has equal weight as support from substantial stimuli. The substantial pair gives support with the strength of it's associative values to border and chunking respectively. Types adds support for one of the two, as typings will support either chunking or border. If one type is involved, it's associative strength to the word will be added to the support for the behaviour it implies. If two types are involved, an average of their strength will be added to the support for the relevant b.
#12. When a chunked type is inherited to next round, it will have the average value of the previous type1 and type2, mening that later typings will have a greater impact on the chunked type's value than the previous ones if more than one chunking is made. 
#13. For simplicity, in this version only the typings of single words will be saved in long-term memory. inherited types will contribute to decisions, but are not saved. This can easily be changed in the future.
#14.Type compatibility: In this version only the lowest level expectation (backwards or forward) is searched for. I.e. for type1 a/b/c, only c or c/something is searced for in type2
#
#  UNSOLVED PROBLEMS OR QUESTIONS
#1. How do we treat ghost types? If s1 is a/b/c and s2 is b/c, they will reduce to a. but what about c? It is expected for full reduction but all of a sudden the expectation disappears. c becomes a ghost type. Maybe, in order to avoid ghost types, new types (assigned by the other type), should be either 1 primitive expected by the other type, or expecting the other type only if the other type is 1 primitive. Currently only the second is the case. If to apply this stricly, the same thing should apply for type compatibility and type reduction.
#2. Reinforcement of type2? Currently not reinforced if border is placed. If the border was good, type2 will be tested and reinforced in the next round. If border was unsuccesful, we know that s2 is not a good first element (at least if the next sentence contition is applied), but how do we generalize this as a filter?
#3. How should types support chunking? Which subchunking should type support. This needs to be thought of. Currently quick dirty solution is to let compatible types give support to all chunking behaviours.

#  STUPID THINGS
# Inherited type is now created independently if border is placed or not, unnecessary computation. It should only be created if response is chunking. Fix later.
#------------------------------------------------------------
#                       Parameters
#------------------------------------------------------------
#LEARNING MECHANISM
beta = 1
alpha = 0.1
#initial values for behaviours  
initial_value_chunking = -1
initial_value_subchunking = -1
initial_value_border = 1
initial_value_types = 0 #the initial value of a type has to be 0 in current model, and it has to receive reinforcement in the same trial as it emerges. if a unit has no other types, then this type will contribute positively or negatively to next similar situation only if initial value is 0. because other decisions than the one supported by the type will recieve support 0 from the type side.
limit_value_for_bad_type=-0.5 #very imortant parameter that governs diversity. if two stimuli only have bad compatible types, nwe primitives can emerge. thus, if this value is high - high diversity and if low, low diversity.
negative_reinforcement = -2
positive_reinforcement = 3
#mechanism variant
subchunk_learner = 1                                                           #subchunk learner considers all chunking possilbilities. else: superchunk or border are the only possible behaviours
type_learner = 1                                                               #ATTENTION: subchunk learner and type learner are not compatible! at least 1 of them has to be 0. See point 10 in coices above.
#SIMULATION
n_trials = 4000
sentences = n_trials*2
simulation_runs = 15



#DATA
n_subjects=10
n_objects=10
n_transitive_verbs=0
n_intransitive_verbs=10
n_verbalized_nouns=0 #are always intransitive in this version
n_subordinate_clause_init=0
n_adjectives=0
max_n_subordinate_clauses=0
p_subordinate_clause_init=0
p_subordinate_clause=0
p_verbalized_noun=0
p_adjective=0
p_intransitive_verb=1
zipf_distribution=False

#parser testing
n_tests=20
maxlength_test=4
n_test_elements=3   
threshold=4                                                         

#------------------------------------------------------------
#                       functions for data and learning
#------------------------------------------------------------
#RESPONDING AND LEARNING

def respond(stimuli,reinforcement,s1,s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types,border):  #function for both respond and learn
    print("NEW TRIAL")
    w=[]
    b_values=[]  
    s2=stimuli[s2_index]                                                   
    pair=[s1,s2]
    unnested_stimulus=[]                                                       
    insert_s_in_memory(sr_value_pairs,pair,s2,unnested_stimulus)               #unnested_stimulus is a list containing all subpairs after each other as strings that can be keys.                                                                                 
    #print(unnested_stimulus)
    #print(sr_value_pairs)
    if subchunk_learner == 1:
        for b in range(len(sr_value_pairs[unnested_stimulus[0]])):             #collect value for each behaviour in repertoire including all subchunking possibilities
            z = 0                                                              #z is a support collector. for each b, several stimuli can give support. so they need to be added up. in order no to get skewed weights, support is collected twice, once from superchunk and once from eventual relevant subchunk, or from superchunk again. this is where I cheat and go only one level for each subchunk. the consistent way is to collect from all relevant subchunks and as b go up, keep adding one more round for the b:s that cat collect from as many chunks. but I keep cheating for now. But, importantly, this means support from types needs two go in *2 if support from substantial stimuli and types is to have equal weight.
            z += sr_value_pairs[unnested_stimulus[0]][b]                       #always collect support from superchunk, i.e., first element in unnested list. all b:s collect support twice, for fairness, from superchunk and from one subchunk, if relevant.if there are no subchunks, support will be collected twice for each b
            if b==0: z += sr_value_pairs[unnested_stimulus[0]][b]              ###SUPPORT FROM SUBCHUNKS if b is border, only support from superchunk is relevant bc identification of complete sentence.
            if b > 0: z += sr_value_pairs[unnested_stimulus[b-1]][1]           ###SUPPORT FROM SUBCHUNKS if e is superchunk or any level of subchunk
            b_values.append(z)                                                 #append value for each response to reuse in learn
            #w.append(exp(beta*z))                                             #append support for each behaviour
    else:
        for b in range (2):    #INCLUDE IF SUBCHUNKING IS NOT INCLUDED, SIMPLE INCREMENTAL LEARNER 
            z = 0
            z += sr_value_pairs[unnested_stimulus[0]][b]
            b_values.append(z)                                                 #append value for each response to reuse in learn
            #w.append(exp(beta*z))                                             #append support for each behaviour
    if type_learner == 1:
        ts1,ts2,b_values,sr_values_types,typatory,assigned_types,inherited_type1,type1=assign_types(b_values,s1,s2,inherited_type1,border,sr_value_types,typatory,assigned_types)    #OFÄRDIGT: denna funktion måste assigna types, uppdatera sr_value_types och typatory, samla typdata för learn och spotta ut en ny b_values där typernas support lagts till
        #ts1,ts2,b_values,sr_value_types,typatory,assigned_types,inherited_type1,type1                                                                        
    for i in range(len(b_values)):
        w.append(exp(beta*b_values[i]))
    q=random()*sum(w)                                                          #random value from sum of support
    response=0
    while q>sum(w[0:response+1]): response+=1                                  #decision. index for behaviour from repertoire        
    pairs.append(unnested_stimulus[0])                                                         #data for learn
    responses.append(response)                                                 #data for learn
    if response > 1: ###FEEDBACK TO SUBCHUNKS
        border=False
        print("response: subchunking")
        pairs.append(unnested_stimulus[response-1]) ###FEEDBACK TO SUBCHUNKS   
        responses.append(1) ###FEEDBACK TO SUBCHUNKS
    s1_index+=1                                                                #s1_index only grows for saving data for learn as above
    s2_index+=1                                                                #s2_index grows for chosing next stimuli in next round
    if response > 1:                                                           #response is some level of subchunking
        chunk_start=s1                                                         #to be filled with subunit to chunk with and s2
        prechunks=[]
        for i in range(response-1):                
            prechunks.append(chunk_start[0])                                   #collect material to fill in what comes before chunk
            chunk_start=chunk_start[1]                                         #loops into nested list to the level that corresponds to response
        chunk=[]                                                               
        chunk.append(chunk_start)
        chunk.append(s2)
        #print(chunk) #yes, chunken är fixad!!
        for i in range((len(prechunks)-1),-1,-1):                              #this backwards loop uses prechunks to create the correct nested list to precede chunk folloed by chunk. the first element in list will be s1 for next round
            if i==len(prechunks)-1:
                prechunks[i]=[prechunks[i],chunk]
            else:
                prechunks[i]=[prechunks[i],prechunks[i+1]]
        s1=prechunks[0] 
        return respond(stimuli,reinforcement,   s1,     s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types,border)                                                             
    if response==1:
        border=False                                                           #passes information to next round's typing on this not being a post-border situation.
        print("response: superchunking")
        return respond(stimuli,reinforcement,   [s1,s2],     s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types,border)  #s2 forms subchunk with last atom from s1 which is subchunk1. the new chunk including the subchunk is passed os as s1 in next round.
    if response==0:
        border=True
        print("response: border")
        s1=s2
##learn: if boundary is placed, all responses that led to this are updated according to u for boundary placement    
        u = reinforcement[s1_index-1]                                          #-1 because index increases after each decision. we want reinforcement to reflect index that was base for placing boundary.
        if s1_index>1:                                                         #control only occurs when more than one pair has been encountered.
            for i in range(s1_index-2-controller):                             #check reinforcement since last border was placed
                if reinforcement[s1_index-2-i]==positive_reinforcement:        #if there should have been a border and it was not placed - no sucess
                    u = negative_reinforcement                                 
            if reinforcement[controller]==negative_reinforcement:              #controlls that positive reinforcement only is given if a complete sentence is identified.
                u = negative_reinforcement
        controller=s1_index-1                                                  #makes sure controller updates to be equal to s1_index-1, which corresponds to last border suggestion.
        if u==positive_reinforcement:
            success=1
            print("positive reinforcement")
        if u==negative_reinforcement: 
            success=0
            print("negative_reinforcement")
        for i in range(len(pairs)):
            unit = pairs[i]
            delta = alpha * (u - sr_value_pairs[unit][responses[i]])
            sr_value_pairs[unit][responses[i]] += delta
        if type_learner == 1:
            if len(assigned_types)>0:
                assigned_types.pop()                                           #this removes the last element of the assigned_types list. this is because we don't want to reinforce the typing of s2 once border is placed, as s2 will be typed in the next round and eventually reinforced.
            sentence_condition=False #only reinforce assigned types if they reduce to sentence
#original emergence of types and reinforcement              
            if u == positive_reinforcement:
                if not ts1:                              
                    #print("popped assigned types",assigned_types)
                    if len(assigned_types)==1:                                 #means no previous chunking has been made i.e. rule 2 applies. s1 shoulod be typed as s.
                        #print("length of assigned types is 1 and stimulus will be typed sentence")
                        assigned_types[0][1]=[0]                 #if no chunking has been made, s1 is the first and only element of assigned types.
                        ts1=True
                    elif len(assigned_types)==2:                               #means one previous chunking and rule 2 applies
                        #print("length of assigned types is 2 and types will be assigned")
                        #assigned_types,sentence_condition,ts1=assign_two_types_that_reduce_to_x(0,assigned_types,ts1)
                            #här finns det ett alternativ som är att välja de s-kompatibla typerna och köra in dem i choose compatible pair
                            #sentence_compatible_types
                        if len(assigned_types[0])==2 and len(assigned_types[1]) ==2 and len(assigned_types)==2:
                            print("assigned types", assigned_types)
                            print("error: s1 is not typed but both the two elements of s1 are typed") #DEBUG. see def assign_types: both stimuli can only be typed if they are compatible and if so ts1 will be true in next round.
                        elif len(assigned_types[0])==2 and len(assigned_types[1])==1:  #first element typed, second not.
                            if len(assigned_types[0][1])==1:                     #if the type of the first element is a primitive, the second element be type1\s
                                assigned_types[1].append([assigned_types[0][1][0],999,0])
                                ts1=True
                                sentence_condition=True
                                #print('sc 1 true, assigned types',assigned_types)
                            elif len(assigned_types[0][1])==3 and assigned_types[0][1][:2]==[0,888]:#if type of first element is s over something
                                assigned_types[1].append([assigned_types[0][1][2]]) #type of second element should then be something
                                ts1=True
                                sentence_condition=True
                                #print('sc 2 truee assigned types',assigned_types)
                        #if the above cases are not true we don't care about one being typed. that typing is wrong because it can't make the two elements a sentence, and they are.
                        elif len(assigned_types[0])==1 and len(assigned_types[1]) ==2:  #first element not typed, second is
                            if len(assigned_types[1][1])==1:                     #if the type of the second element is a primitive, the first element be s/type2
                                assigned_types[0].append([0,888,assigned_types[1][1][0]])
                                ts1=True
                                sentence_condition=True
                                #print('sc 3 true assigned types',assigned_types)
                            elif len(assigned_types[1][1])==3 and assigned_types[1][1][1:]==[0,888]:#if type of second element is something under s
                                assigned_types[0].append([assigned_types[1][1][0]]) #type of first element should then be something
                                ts1=True
                                sentence_condition=True
                                #print('sc 4 true, assigned types',assigned_types)
                        else:    
                        #elif len(assigned_types[0])==1 and len(assigned_types[1]) ==1:  #no element is typed, two alternatives will emerge; s, s\s och s/s, s. båda ska in i ltm, men ett dras till assigned types
                            type1a=[0]
                            type2a=[0,999,0]
                            type1b=[0,888,0]
                            type2b=[0]
    #                            if json.dumps(type1a) not in typatory:             #introduce all types in typatory if they are not there
    #                                    typatory[json.dumps(type1a)]=initial_value_types
    #                            if json.dumps(type1b) not in typatory:
    #                                    typatory[json.dumps(type1b)]=initial_value_types
    #                            if json.dumps(type2a) not in typatory:
    #                                    typatory[json.dumps(type2a)]=initial_value_types
                            if random()>0.5:
                                assigned_types[0].append(type1a)               #assigned_types will be introduced in ltm (if they are not there) when reinforced
                                assigned_types[1].append(type2a)
    #                                sr_value_types[json.dumps(assigned_types[0][0])]=dict()
    #                                sr_value_types[json.dumps(assigned_types[0][0])][json.dumps(type1b)]=initial_value_types #add the other possible type to the types of the stimuli.
                                
    #                                sr_value_types[json.dumps(assigned_types[1][0])]=dict()
    #                                sr_value_types[json.dumps(assigned_types[1][0])][json.dumps(type2b)]=initial_value_types #add the other possible type to the types of the stimuli.
                                
                            else:
                                assigned_types[0].append(type1b)
                                assigned_types[1].append(type2b)
    #                                sr_value_types[json.dumps(assigned_types[0][0])]=dict()
    #                                sr_value_types[json.dumps(assigned_types[0][0])][json.dumps(type1a)]=initial_value_types #add the other possible type to the types of the stimuli.
    #                                sr_value_types[json.dumps(assigned_types[1][0])]=dict()
    #                                sr_value_types[json.dumps(assigned_types[1][0])][json.dumps(type2a)]=initial_value_types #add the other possible type to the types of the stimuli.
                            ts1=True
                            sentence_condition=True #now, in one way or another, the assigned types reduce to s
    #                            print('sc 5 true, assigned types',assigned_types)
                    #print("assigned types",assigned_types,"inherited_tpe1",inherited_type1)
#reinforce types in sr_value_types and typatory                 
                print("ts1",ts1)
                if ts1:                                                            #only reinforce types if s1 is typed, by itself or as an inherited type. otherwise, the typings in s1 have not contributed to border placement
                    if type1!=0 and type1[0]==[0]: #if s1 is a sentence, chunked or not
                        print("typ1",type1)
                        sentence_condition=True
                        print("sc 5",sentence_condition)
                        print("at",assigned_types)
                    if sentence_condition and assigned_types[1][1]==888:
                        print("ERROR wrong intransitive verb type")
                        return
                    if sentence_condition:                                         #reinforcement of types only occurs if types reduce to s and thus can be said to have contributed to the border placement decision
                        print("types get positive reinforcement")
                        for i in assigned_types:
                            if len(i)>1:
                                if json.dumps(i[0]) not in sr_value_types:                 #first, make sure that stimuli are in ltm (they will not be if they are new)
                                    sr_value_types[json.dumps(i[0])]=dict()
                                if json.dumps(i[1]) not in sr_value_types[json.dumps(i[0])]:  # and, even if stimulus are in ltm, the assigned type may not be, so make sure it is
                                    sr_value_types[json.dumps(i[0])][json.dumps(i[1])]=initial_value_types
                                    #print(sr_value_types[json.dumps(i[0])][json.dumps(i[1])])
                                if json.dumps(i[1]) not in typatory:
                                    typatory[json.dumps(i[1])]=initial_value_types
                                delta = alpha * (u - sr_value_types[json.dumps(i[0])][json.dumps(i[1])]) # calculate delta for sr_value_types and typatora and updated the type values. can only be done if typing is made, thus only of i<1
                                #print("delta for updating sr-value types:", delta)
                                sr_value_types[json.dumps(i[0])][json.dumps(i[1])] += delta
                                delta = alpha * (u - typatory[json.dumps(i[1])])
                                #print("delta for updating typatory:", delta)
                                typatory[json.dumps(i[1])] += delta
                    #else: u=negative_reinforcement
                    #print("assigned types that have been updated",assigned_types)
                    #print("updated sr_value_types",sr_value_types)
                    #print("updated typatory",typatory)
            elif u==negative_reinforcement and ts1:
                if type1[0]==[0]: #only reinforce a bad border negatively if types reduce to s and thus support border placement
                    print("types get negative reinforcement")    
                    for i in assigned_types:
                            if len(i)>1:
                                if json.dumps(i[0]) not in sr_value_types:                 #first, make sure that stimuli are in ltm (they will not be if they are new)
                                    sr_value_types[json.dumps(i[0])]=dict()
                                if json.dumps(i[1]) not in sr_value_types[json.dumps(i[0])]:  # and, even if stimulus are in ltm, the assigned type may not be, so make sure it is
                                    sr_value_types[json.dumps(i[0])][json.dumps(i[1])]=initial_value_types
                                    #print(sr_value_types[json.dumps(i[0])][json.dumps(i[1])])
                                if json.dumps(i[1]) not in typatory:
                                    typatory[json.dumps(i[1])]=initial_value_types
                                delta = alpha * (u - sr_value_types[json.dumps(i[0])][json.dumps(i[1])]) # calculate delta for sr_value_types and typatora and updated the type values. can only be done if typing is made, thus only of i<1
                                #print("delta for updating sr-value types:", delta)
                                sr_value_types[json.dumps(i[0])][json.dumps(i[1])] += delta
                                delta = alpha * (u - typatory[json.dumps(i[1])])
                                #print("delta for updating typatory:", delta)
                                typatory[json.dumps(i[1])] += delta
                    
            assigned_types.clear()                                          #empties the assigned_types dictionary after reinforcement has been given to assigned types, so that it will start ampty in next round
            inherited_type1=0 #makes sure ihtt1 inte ligger och skvalpar i minnet
        return s1,s1_index,s2_index,[],[],sr_value_pairs,success,controller,inherited_type1,assigned_types,border
        
#sr_value_pairs=dict()
#s2=9
#unnested_stimulus=[]
def insert_s_in_memory(sr_value_pairs,pair,s2,unnested_stimulus):
    hash = json.dumps(pair)
    unnested_stimulus.append(hash)
    if hash not in sr_value_pairs:
        initial_values = [initial_value_border]                                #sr_value[0]=border
        chunking_possibilities=1                                               #chunking possibilities correspond to nodes that are accessible for s2, defined below
        chunk_counter=pair[0]
        while (type(chunk_counter)==list):            
            chunk_counter=chunk_counter[1]
            chunking_possibilities+=1                                          #counts the number of acessible nodes or subchunks
            #print(chunking_possibilities)
        for i in range(chunking_possibilities):
            initial_values.append(initial_value_chunking)    
        sr_value_pairs[hash] = initial_values 
    if (type(pair[0])==list):
        insert_s_in_memory(sr_value_pairs,[pair[0][1],s2],s2,unnested_stimulus)
    return sr_value_pairs, unnested_stimulus
#sr_value_pairs, unnested_stimulus=insert_s_in_memory(sr_value_pairs, [[1,[4,3]],9],s2,unnested_stimulus)            
#print(sr_value_pairs, unnested_stimulus[1])
    
def assign_types(b_values,s1,s2,inherited_type1,border,sr_value_types,typatory,assigned_types):  #denna funktion måste assigna types, uppdatera sr_value_types och typatory, samla typdata för learn och spotta ut en b_values där typernas support lagts till
#check if stimuli have good types (value above 0) and if s1 has at least one good type compatible with position 1. if not, there is nothing to generalize, and no typing will be made. the exclusion of types with value below 0 is a bit harsh, means if a type fails once it is out. it is a simple choice for the pilot. for example it avoids a situation where a type with a negative value favours a correct decision (e.g. a noun typed as s bc verb is typed s\s and contributes to chunking bc the type has a negative value) and is reinforced. this will lead to bouncing around zero and is weird. so I do this for simplicity and see how it works. excuding bad types is also important for the emergence of new types and more so the emergence of new primitives.
    ts1=True                                                                   #value true for ts1 and ts2 respectively means typings can be made.
    ts2=True
    types2=dict()                                                              #assigns a value to variables so that they can go into function even if s1 or s2 is not in sr_value_types
    types1=dict()
    good_types1=dict()
    s1string=json.dumps(s1)
    s2string=json.dumps(s2)
    if border:                                                                 #if there is no chunked s1 i.e. last response was a border or it is the first round (if there is a chunked s1 it will already be typed, i.e. type1 is not 0)
        if s1string not in sr_value_types: 
            ts1=False                           #means there are no types for s1
            #print("s1",s1, "is not in ltm")
        else: 
            #print("s1",s1, "is in ltm")
            #print(s1,"has types",sr_value_types[s1string])
            #print(sr_value_types)
            types1=sr_value_types[s1string]                                    #in the dictionary sr_value_types, the value side needs to be a dictionary containing all possible types for this stimulus and the value of each
            good_types=dict()                                                  #first, sort out the good types that have a value of 0 or more
            good_types1=dict()
            for i in types1:
                if types1[i] >= limit_value_for_bad_type:
                    good_types[i]=types1[i]
            good_types1 = ct_s1_unconstrained(good_types)                      #(compatible types for position 1) defines a subgroup of good types for s1 that are compatible with position 1
            #print("good_types1",good_types1)
            if len(good_types1) == 0:                                          #no compatible and good types are found
                ts1=False                                                      #if no good type for s1 is compatible with position 1 there will be a cognitive syntax error and it will count as s1 not having a type
                #print("ts1 false because of no good types")
    if not border and inherited_type1==0:                                                #if a chunking has been made but no typing, for simplicity, no typings will be made. futurewise, type of s2 may assign types to previous stimuli. but this is complicated. and cognitively, this woiuld mean I recognize a string but can't parse it, but when another element turns up I can parse it. a bit weird.
        #print("no inherited type 1")
        ts1=False
        type1=0
        #ts2=False
    if ts2:                                                                    #only go on if there is not a chunked untyped s1
        if s2string not in sr_value_types: 
            ts2=False
            #print("s2",s2,"is not in ltm")
        else: 
            #print("s2",s2,"is in ltm")
            #print(s2,"has types",sr_value_types[s2string])
            types2=sr_value_types[s2string]                   
            good_types2=dict()
            for i in types2:
                if types2[i] >= limit_value_for_bad_type:                                          #if the v-value of the type is 0 or more it will be added to good types2. we will only work with good types. but bad types need to be kept associated to the stimulus. because when new types emerge we need to make sure they are not identical to a bad type. this will be a way to get diversity and introduyce other primitives than s.
                    good_types2[i]=types2[i]
            #print("good types 2",good_types2)
            if len(good_types2)==0:
                ts2=False                                                      #if there are no good types for s2, it will count as s2 not having types.
            #print("ts2",ts2) 
    if ts1==False and ts2==False: 
        type1=0
        if border:
            #print("no types post border")
            assigned_types=[[s1],[s2]] 
            #print("assigned types",assigned_types)
        else:
            #print("no types not post border")
            assigned_types.append([s2])
            #print("assigned types",assigned_types)
        return  ts1,ts2,b_values,sr_value_types,typatory,assigned_types,inherited_type1,type1     #if no element has a good and compatible type, no typing is applied            
#typings will be made
    elif ts1 and not ts2:                                                      #types1 has at least one compatible and good type (or there is an inherited chunked and typed s1 from previous round), but s2 has none
        #print("type 1",type1)
        if border:                                                             #there is no chunked s1 i.e. last response was a border. if not, for simplicity in the pilot, type1 will always remain. s2 will be typed accordingly if possible.
            print("type 1 comes from choose_type")
            type1=choose_type(good_types1)                                     #probabilistic function that chooses type based on the associative values of the types            
        else: 
            type1=inherited_type1
            types1[json.dumps(type1[0])]=type1[1]
            good_types1[json.dumps(type1[0])]=type1[1]
            print("type1 is inherited")
            print("new type 2 is assigned by type 1")
        type1,type2,typatory,ts2=new_type2(type1,types1,types2,typatory,border,assigned_types)    #assigns the simplest type2 that is compatible with type1 (1. winning compatible type from typatory. 2. new type emerges. 3. both types are modified, onlyt applies if border=1)           
    elif ts2 and not ts1:                                                      #only s2 has at least one good type
        print("type 2 comes from choose type")
        type2=choose_type(good_types2)        
        if border:
            print("type 1 is a new type assigned by type 2")
            type1,type2,typatory,ts1=new_type1(type2,types2,types1,typatory,border,assigned_types) #type1 will be invented and type 2 will be modified if necessary. only if s1 is not complex, see rule 8 avove.            
    elif ts1 and ts2:                                                          #there are good types for both position, and a winning compatible pair will be chosen, or if necessary a new type will be assigned or emerge
        # if not border:
        #     type1=inherited_type1
        #     types1[json.dumps(type1[0])]=type1[1]
        #     good_types1[json.dumps(type1[0])]=type1[1]
        #     print("type1 is chunked:",type1)
        #     print("type 1 and 2 come from choose compatible pair (a)")
        #     type1,type2,typatory=choose_compatible_pair(types1,types2,good_types1,good_types2,typatory,border,assigned_types) 
        if border:                                                             #both s1 and s2 have at least one good type
            print("type 1 and 2 come from choose compatible pair (b)")
            type1,type2,typatory=choose_compatible_pair(types1,types2,good_types1,good_types2,typatory,border,assigned_types) #if postborder all types will be scrambled to find a compatible pair. if none is found, a dominant type will be chosen and the other one will be invented.            
        else:                                                                  #if s1 is chunked and typed (and reduced), no changes will be made to type1 in pilot. garden-path-retypings is for future versions.
            type1=inherited_type1
            print("s1 is chunked and typed and type2 will adapt. type1:",type1)
            compatible_types2=ct_s2_constrained_by_s1(type1,good_types2)          #a list is made of the good types for s2 that are compatible with type1.
            if len(compatible_types2)>0:                                       #if there is at least one compatible and good type for s2, choose one
                print("existing type 2 is assigned by chunked and typed s1")
                type2=choose_type(compatible_types2)                
            else:                                                              #otherwise a new type will be invented for s2. s1 will not be modified, this is stopped in the new type function.
                print("a new type 2 is assigned by chunked and typed s1")
                type1,type2,typatory,ts2=new_type2(type1,types1,types2,typatory,border,assigned_types)
    if type1==0: ts1=False                                                     #in the new types functions ts is not updated, but types are returned as 0 if the conditions for assigning types are not present. so ts needs to be updated here before decision support is collected and learning occurs
    if type2==0: ts2=False
#now, typings are made. update the assigned_types list with s1 (if border) and s2 (always), and eventual assigned types. if border is placed, the last element of this list will always be removed, because type reinforcement (and original emergence) should only be applied to elements in the ideintified sentence. s2 will be typed and reinforced in the next round.
    if border:
        assigned_types=[[s1],[s2]]                                             #in a post-border situation, both stimuli should be added to assigned types. they should be inserted in a list that will eventually also contain its type, if it is types
        print("typing post border")
        #print(assigned_types)    
        if ts1:
            print("type 1" ,(type1))
            assigned_types[0].append(type1[0])                                    #if s1 is typed, add its type to the s1. only in post border. assigned types hould only contain single stimuli and their typings, see rule 13.
    else:
        print("typing not post border")
        assigned_types.append([s2])                                            #in a non-post-border situation, the elements of s1 are already in the assigned_types list, so only s2 list should be added to assigned_types.
    if ts2:                                                                    #if s2 is typed, its type should always be added to the type2 list, indepently of the border status.
        print("type 2",(type2))
        assigned_types[len(assigned_types)-1].append(type2[0])
        print("assigned types",assigned_types,"type1",type1,"type2",type2)
        #if ts1 and len(assigned_types)==3: #a previous typing has been made and it is possible that it needs to be updated to a new type1
        #if type1!=inherited_type1:
            #print("type1 has been modified and is no longer inherited type, which is", inherited_type1)
#om assigned types är mer än en lång (egentligen mer än noll) så är vi inte postborder och vi kan ha a/b,b elkler a\b,b innan. i första fallet måste a ändras till primitive. i andra fallet måste b ändras till primitive
            #print("new design type 1 modification, assigned types pre" , assigned_types, "type2:",type2)
            #assigned_types,sentence_condition,ts1=assign_two_types_that_reduce_to_x(type1[0][0],assigned_types,ts1) #the previous types should reduce to the first element in type 1#Nmakes sure the two previouslu assigned types are changed to reduce to primitive
            #if len(type1[0])>1: 
                #for i in range(len(type1[0])-1):
                    #assigned_types[1][1].append(type1[0][i+1])   #and the last type of appended types also needs to be modified, because new type1 is primitive over type two, so that is what the two elements should reduce to
                #print("assigned types post" , assigned_types)
                
            

#now,one or two types are assigned and typings will give support to a behaviour
#om type1 är sentence så ska border stödjas med type1:s styrka OAVSETT om s2 är typad eller inte, OM INTE type2 börjar på s\, isf ska de chunkas. type 2:s styrka bidrar inte till border även om den är kompatibel med s och border, dvs är kompatibes med position 1.
#om type1 o type2 är chunk-kompatibla så  ska chunkning stödjas. Då läggs medelvärdetvärdet av typ 1 o 2 på chunk.
#om type1 och type2 är chunk-kompatibla så ska en inherited type_1 som ärvs till nästa runda skapas. den kan bara skapas här eftersom compatible_type kan ha valts probabilistiskt bland flera möjliga och därför "vet" vi inte förrän här vilak typ1 och typ2 är och hur de är kompatibla.    
    
    
    if ts1 and ts2:
        print("create inherited type")
        print("type1",type1,"type2",type2)
        match=False        
        if type1[0]==[0] and (len(type2[0])<2 or type2[0][:2]!=[0,999]):       #if type1 is a sentence, and type2 does not take a preceding sentence as an argument, types will support border, independently of whether s2 is typed or what its type is.:
            #print("type1[0]==[0]:")                
            b_values[0]+=type1[1]                                                  #add type1 value to behaviour border
            inherited_type1=0                                                      #even if s2 is typed and border is placed, type1 should not be inherited in next round. it should start from beginning so that compatibility is checked with all typs for s2 in next round. we don't want a fixed inherited type1 in this case.
            #print("inherited type is none because type 1 is sentence and type 2 does not take a sentence as an argumenbt")
        else: #if type 1 is not a sentence, or if it is and type 2 does not take a sentence as an argument        
            for i in range((len(type1[0])-1),-1,-1):                           #bacwards loop throuhg type1. 
                if type1[0][i]==888:                                           #börja med att kolla efter / i type1
                    #print("OVER FOUND in type 1")
                    if len(type2[0])>=len(type1[0][i+1:]):                        #om man identifiewrat en chunk i type 1 som letar efter överlapp så måste type2 vara minst så lång för att det ska finnas
                        #print("length of type 2 is at least as long as what comes after over in type1")
                        #print("type1[0][i+1:]:::",type1[0][i+1:],"type2[0][:len(type1[0][i+1:])]:::",type2[0][:len(type1[0][i+1:])])
                        if type1[0][i+1:]==type2[0][:len(type1[0][i+1:])]:         #om vi hittar ett överlapp
                            #print("there is a match")
                            inherited_type1nv=type1[0][:i]                       #det som föregår / är det sopm blir kvar. nv står för no values. de läggs på sist.
                            if len(type2[0])>len(type1[0][i+1:]):              #om det finns mer än det som ingick i reduktionen i type2
                                for k in type2[0][len(type1[0][i+1:]):]:         #loopa igenom det som återstår i type2 och appenda dem 1 o 1 till den ärvda typen, för att inte appenda en lista, som då blir underordnad resten
                                    inherited_type1nv.append(k)
                            match=True 
                            #print("second match true")
                            break
            if not match:                                                      #om inget överlapp hittas baserat på / i type1 går vi vidare och letar efter \ i type2 och följer samma procedur som ovan fast spegelvänd.
                #print("JAG ÄR HUR")
                for j in range(len(type2[0])):
                    if type2[0][j]==999:
                        if len(type1[0])>=len(type2[0][:j]):
                            #print("len is fine")
                            #print(type1[0][len(type1[0])-len(type2[0][:j]):], "==" ,type2[0][:j] )
                            if type1[0][len(type1[0])-len(type2[0][:j]):] == type2[0][:j]:                                
                                inherited_type1nv=type2[0][j+1:]
                                if len(type1[0])>len(type2[0][:j]):
                                    for l in range((len(type1[0])-len(type2[0][j+1:])-1),-1,-1):
                                        inherited_type1nv.insert(0,l)
                                match=True
                            else:
                                print("ERROR both elements have been typed and should be compatible but they are not") 
                                break
            #print("inherited type1 no values" ,inherited_type1nv)
            inherited_type1=[inherited_type1nv,(type1[1]+type2[1])/2]
            print("inherited type with values",inherited_type1)
            if match: #should always be true, be check för säkerhets skull
                for i in range(len((b_values[1:]))):
                    b_values[i+1]+=(type1[1]+type2[1])/2                       # if compatible, add avarage of type1 and type2 value to each chunking behaviour. to figure out which chunking types actually support is too hard of a problem to solve in this version, wait until tranferred to jeromes version  
    elif ts1 and not ts2:
        inherited_type1=0    
        if type1[0]==[0]:
            b_values[0]+=type1[1]                                                  #add type1 value to behaviour border
        else: print("ERROR s1 is typed and not typed sentence and s2 is still not typed")
#          for i in range(len(b_values[1:])): #SKIP, if not both are typed and compatible, types should not support anything
#            b_values[i+1]+=type1[1]                                         #add value of type1 to each chunking behaviour. to figure out which chunking types actually support is too hard of a problem to solve in this version, wait until tranferred to jeromes version  
    elif ts2 and not ts1:                                                      #if only s2 is typed, no behaviour support will be given, for simplicity. can be changed in future versions. unsure of what is best.
        inherited_type1=0
        if border:print("ERROR s2 is typed in a postborder state and s1 is still not typed. s2:",type2)
    else: print("ERROR supposedly typings have been made but no ts is true")
    return ts1,ts2,b_values,sr_value_types,typatory,assigned_types,inherited_type1,type1            #ts1 och ts2 is to check in decision function if typings have been made. b_values are updated b_values that have weighted in support for one of the behaviours that is supported by types. sr_value_types and typatory have been updated if new types have been invented. assigned_types contains the assigned types and will be used as a reference to update their values in sr_value_types in the learn function. if a chunking is made, they will reduce to next round's type1.

                    
def assign_two_types_that_reduce_to_x(x,assigned_types,ts1):
        #här finns det ett alternativ som är att välja de x-kompatibla typerna och köra in dem i choose compatible pair
        #sentence_compatible_types
    sentence_condition=False
    if len(assigned_types[0])==2 and len(assigned_types[1]) ==2:   #if both elements are types
        print("error: s1 is not typed but both the two elements of s1 are typed") #DEBUG. see def assign_types: both stimuli can only be typed if they are compatible and if so ts1 will be true in next round.
        if len(assigned_types[0][1])>2 and assigned_types[0][1][1]==888: #If first element is something over second element
            assigned_types[0][1][0]=x #someting should be x
        elif len(assigned_types[1][1])>2 and assigned_types[1][1][len(assigned_types[1][1])-2]: #if second element is first element under something
            assigned_types[1][1][len(assigned_types)-1]=x #something should be x
            ts1=True
            if x==0: sentence_condition=True
    elif len(assigned_types[0])==2 and len(assigned_types[1])==1:  #first element typed, second not.
        if len(assigned_types[0][1])==1:                     #if the type of the first element is a primitive, the second element be type1\x
            assigned_types[1].append([assigned_types[0][1][0],999,x])
            ts1=True
            if x ==0: sentence_condition=True
            #print('sc 1 true, assigned types',assigned_types)
        elif len(assigned_types[0][1])==3: 
            assigned_types[0][1][:2]=[x,888]  #type of first element should be x over something
            assigned_types[1].append([assigned_types[0][1][2]]) #type of second element should then be something
            ts1=True
            if x ==0: sentence_condition=True
            #print('sc 2 truee assigned types',assigned_types)
    #if the above cases are not true we don't care about one being typed. that typing is wrong because it can't make the two elements a sentence, and they are.
    elif len(assigned_types[0])==1 and len(assigned_types[1]) ==2:  #first element not typed, second is
        if len(assigned_types[1][1])==1:                     #if the type of the second element is a primitive, the first element be x/type2
            assigned_types[0].append([x,888,assigned_types[1][1][0]])
            ts1=True
            if x ==0: sentence_condition=True
            #print('sc 3 true assigned types',assigned_types)
        elif len(assigned_types[1][1])==3:
            assigned_types[1][1][1:]=[999,x]  #itype of second element should be something under x
            assigned_types[0].append([assigned_types[1][1][0]]) #type of first element should then be something
            ts1=True
            if x ==0: sentence_condition=True
            #print('sc 4 true, assigned types',assigned_types)
    else:    
    #elif len(assigned_types[0])==1 and len(assigned_types[1]) ==1:  #no element is typed, two alternatives will emerge; s, s\s och s/s, s. båda ska in i ltm, men ett dras till assigned types
        type1a=[0]
        type2a=[0,999,x]
        type1b=[x,888,0]
        type2b=[0]
        if random()>0.5:
            assigned_types[0].append(type1a)               #assigned_types will be introduced in ltm (if they are not there) when reinforced
            assigned_types[1].append(type2a)             
        else:
            assigned_types[0].append(type1b)
            assigned_types[1].append(type2b)
        ts1=True
        if x ==0: sentence_condition=True #now, in one way or another, the assigned types reduce to s
        #print('sc 5 true, assigned types',assigned_types)
    return assigned_types,sentence_condition,ts1


def choose_compatible_pair(types1,types2,good_types1,good_types2,typatory,border,assigned_types):    #denna funktion är klar. a winning compatible pair will be chosen, if one can be found in good types, or if a chosen dominant type is compatible with a type in typatory.
    #print("input choose compatible pair: types1",types1,"types2",types2,"good_types1",good_types1,"good_types2",good_types2)
    #type_counter=len(types1)-1+len(types2)                                     #the followinmg choose_dominant_type function will choose one type at a time and see if it is part of a compatible pair. if not, this type will be removed from the scramble and the type counter will go down. if only one type is left it means there is no omptatible pair of types and a strong type will be chosen to assign a type to the other.
    success=False
    first_dominant_type,compatible_type,success,reduced_good_types1,reduced_good_types2=choose_dominant_type(good_types1,good_types2,typatory) # the dominant type from the first round is saved with a unique name so that it can be used if there a no compatible pairs
    if success: dominant_type=first_dominant_type
    else:
        while not success and len(reduced_good_types1)>0 and len(reduced_good_types2)>0: #loop until a compatible pair is found or one of the elements has no types left ie.e there is no compatible pair
            dominant_type,compatible_type,success,reduced_good_types1,reduced_good_types2=choose_dominant_type(reduced_good_types1,reduced_good_types2,typatory) #i scrambeln deltar endast bra typer. NU MÅSTRE JAG VÄLJA DEN BÄSTA TYPEN FRÅN COMP1 O 2 MINUS DEN TESTADE TYPEN FRÅN 1. LÅT OSS SÄGA ATT JAG GÖR EN NY GEMENSAM DICTIONARY FÖR DESSA OCH JAG GER DEM NYA INDEX DÄR T1 BÖRJAR PÅ 1 OCH T2 BÖRJAR PÅ 101
    if success:                                                                #a good compatible pair was found before type counter came to 1. no types will emerge in this round
        if dominant_type[2]==1:                                                #types1 are indexed 1 as third element of list
            type1=dominant_type[:2]                                            #slash list so thet the index from the scramble is excluded. we want typa1 and type2 to com out as lists with two elements, the type and its value.
            print("type 1 is dominant type and is", type1)
            type2=compatible_type[:2]
            print("type 2 is compatible type and is ",type2)
        else:                                                                  #types2 are indexed 2 as third element of list
            type2=dominant_type[:2]                                                
            print("type 2 is dominant type and is", type2)
            type1=compatible_type[:2]
            print("type 1 is compatible type and is", type1)
    else:                                                                      #no compatible pair within existing types. the first chosen type will assign a new type to the other element
        if first_dominant_type[2]==1:                                          #dominant type was from type1list
            type1=first_dominant_type[:2]
            print("type 1 is first dominant type and is", type1)
            new_types=dict()                                                   #new types will collect possible types from typatory (those that are not already in types2 and thus bad types)
            for key in typatory:
                if key not in types2:
                    new_types[key]=typatory[key]
            compatible_new_types=ct_s2_constrained_by_s1(type1,new_types)
            if len(compatible_new_types)==0:                                   #no compatible type was found in typatory.
                type1, type2, typatory, ts2 =new_type2(type1,types1,types2,typatory,border,assigned_types)      #type2 will be invented if no compatible type is in typatory
                print("new type 2 is assigned by first dominant type 1 and is ",type2 )
            else:
                type2=choose_type(compatible_new_types)                        #if at least 1 compatible type is in typatory, one of these will be chosen. OBS the new type will have the same value as in typatory, i.e. its value for all previous use. not sure this is good if value is negative. but negative types in typatory can't be excluded, as they may have been dysfunctional for other kinds of elements but is not necessary so for this one. so it will have to do for now.
                print("new type 2 is from typatory and is", type2)
        else:                                                                  #dominant type was from type2list
            type2=first_dominant_type[:2]
            print("type 2 is first dominat typoe and is", type2)
            new_types=dict()                                                   #same as above will be performed but for type1
            for key in typatory:
                if key not in types1:
                    new_types[key]=typatory[key]
            compatible_new_types=ct_s1_constrained_by_s2(type2,new_types)
            if len(compatible_new_types)==0:                                   #no compatible type was found in typatory.
                type1,type2,typatory,ts2=new_type1(type2,types2,types1,typatory,border,assigned_types)   #type1 will be invented if no compatible type is in typatory
                print("new type 1 is assigned by first dominant type 2 and is ",type1 )
            else:
                type1=choose_type(compatible_new_types)                        #if at least 1 compatible type is in typatory, one of these will be chosen. OBS the new type will have the same value as in typatory, i.e. its value for all previous use. not sure this is good if value is negative. but negative types in typatory can't be excluded, as they may have been dysfunctional for other kinds of elements but is not necessary so for this one. so it will have to do for now.
                print("new type 1 is from typatory and is",type1)
    return type1,type2,typatory                                                #IMPORTANT; TYPES MUST BE LISTS. first place is type, second place is value. this makes them easy to work with 
            
                            
#assign new types, some test variables below, it all works as planned!
#type1=[[1,888,7,999,6],2]                            
#types1=dict()
#types2=dict()
#types2[json.dumps([1, 999, 0])]=0
#types2[json.dumps([7, 999, 6])]=0
#types2[json.dumps([0, 999, 6])]=0
#types1[json.dumps([1,888,1,999,6])]=-1
#typatory=dict()
#border=True
#initial_value_types=0
def new_type2(type1,types1,types2,typatory,border,assigned_types):                            #KLAR! a new type emerges for s2 that is compatible with type1. Alternatively, if everything that can be assigned are bad types, both type1 and type2 will be modified.
#låt oss nu anta att en typ ser ut såhär: [1,888,2] eller [1,999,2]. 888=over /. 999=under \. den blir jsondumpad för att kunna vara en key i en dict. här loadar vi den tillbaka till en lista för att enkelt kunna manipulera den.             
    #print("new type 2 debug. type1",type1,"types1",types1,"types2",types2,"typatory",typatory,"border",border )
    #inherited_type1=0                                                          #if no type can be passed on to next round, this carries that information
#    print(type1)
    ts2=True                                                                   #we start by assuming that s2 can be typed. if not, this will be changed.
    type2=0                                                                   #assign value to type2 so it can be returned even if type not assigned,
    if len(type1[0])==1:                                                       #if type 1 is a primitive, the simplest compatible type is type1\s.  
        if type1[0]==[0]:                                                      #but if type1 is a sentence, type2 is not typed.
            ts2=False
        else:
            primitive=0                                                            
            type2=[[type1[0][0],999,primitive],initial_value_types]            #when a new type is assigned based on the type of the other element, the simplest way is chosen. if the other is a primitive, element will reduce to s (0) with it. if the other element expects sometning, lowest level expectation will be fulfilled.
            type2string=json.dumps(type2[0])                                   #type2 as a key is the first element in list
            while type2string in types2 and types2[type2string]<limit_value_for_bad_type:                                       #this means they are existing bad types
                primitive+=1
                type2[0][2]=primitive                                          #varje gång det inte funkar byter man primitiv i den nya typen, dvs vad den nya typen reducerar till. NYA PRIMITIVER KAN då uppstå!! OBS viktigt!!
                type2string=json.dumps(type2[0])                                   
#            if type2string not in typatory:
#                typatory[type2string]=initial_value_types                      #add this type to typatory if it is not there already                  
    else:                                                                      #if type1 is not a primitive we are only interested in what it expects forward. thus what follows any 888 can be assigned as a type to s2
        for i in range((len(type1[0])-1),-1,-1):                               #backwards loop. this is because we want to test the lowest level expectation first. this favours simplicity and simulates simpler interpretations from the learner
            if type1[0][i]==888:
                type2=[type1[0][i+1:],initial_value_types]                     #slices list and makes what comes after / (bc backwards loop defined as position len(type1[0])-i type2, independently of if it's a primitive or primitive\x
                type2string=json.dumps(type2[0])
                if type2string not in types2:                                  #means  that the suggested type is not an existing bad type and can be assigned to s2.
#                            if type2string not in typatory:
#                                typatory[type2string]=initial_value_types
                    break
                    
            else:                                                          #means that all type1 expectations are bad types for s2. we need to change the lowest level expectation in type 1 by replacing it with a primitive that is not the one currently used
                if border:                                                 #this operation, for simplicity in the pilot, is only done if we are in a post-border situation, i.e. s1 is not chunked.
                    new_primitive=0
                    for i in range((len(type1[0])-1),-1,-1):               #start backwards loop again
                        if type1[0][i]==888:
                            type1[0][i+1]=new_primitive                    #replace lowest level primitive in type 1 with new primitive so we can test if it makes a bad type for s1. it doesn't matter if lowest expected type is primitive or of the type a\b. lowest level primitive will always follow last /. the simplest way to invent a new type is to change the lowest level primitive. in the most common case, s1 will expect a primitive, we know that s2 is not this primitive, so we will give s2 a new primitive and add and assign the type that expects this primitive to s1. both are modified.
                            type2=[type1[0][i+1:],initial_value_types]
                            type1string=json.dumps(type1[0])               #dump it so it can be searched for in types1
                            type2string=json.dumps(type2[0])
                            while (type2string in types2 and types2[type2string]<limit_value_for_bad_type) or (type1string in types1 and types1[type1string]<limit_value_for_bad_type):  #count from 0 and upwards until find a number that is not a bad type for s2 or will make a new bad type for s1
                               new_primitive+=1
                               type1[0][i+1]=new_primitive                 #replace lowest level primitive in type 1 with new primitive so we can test if it makes a bad type for s1. it doesn't matter if lowest expected type is primitive or of the type a\b. lowest level primitive will always follow last /. the simplest way to invent a new type is to change the lowest level primitive. in the most common case, s1 will expect a primitive, we know that s2 is not this primitive, so we will give s2 a new primitive and add and assign the type that expects this primitive to s1. both are modified.
                               type2=[type1[0][i+1:],initial_value_types]
                               type1string=json.dumps(type1[0])            #dump it so it can be searched for in types1
                               type2string=json.dumps(type2[0])
                               #print("LOOPING IN NEW TYPE 2 ")
                               #print("new type 2 debug. type2string",type2string,"types2",types2,"type1string",type1string,"types1",types1)
                            if type1string not in types1:
                                type1[1]=initial_value_types               #if type 1 is modified and not existed before, it will restart at initial value. type2[0] is already defined above.
                            else:
                                type1[1]=types1[type1string]
                    if len(assigned_types)==2: #om assigned types är mer än en lång (egentligen mer än noll) så är vi inte postborder och vi kan ha a/b,b elkler a\b,b innan. i första fallet måste a ändras till primitive. i andra fallet måste b ändras till primitive
                        assigned_types[0][1][len(assigned_types)-1]=new_primitive #update assigned types so that last type is modified to fit change in type1             
#                    if type1string not in typatory:
#                        typatory[type1string]=initial_value_types          #add both new types to typatory if they are not there already. s_r_value_types is updated in assign_types.
#                    if type2string not in typatory:
#                        typatory[type2string]=initial_value_types
    if type2 ==0:ts2=False                                                     #if a type 2 could not be assigned, for example if there is a chunked s1 and s2 can't be typed accordingly, s2 will simply not be typed.
    return type1, type2, typatory, ts2                                             #if double modification has not ocurred, type1 will be returned unmodified. type2 will have been modified if possible (impossible only if s1 is chunked and thus unmodifiable and none of its expectayions is a good type for s2), or else be "no".

#TEST NEDAN
#type1, type2, typatory =new_type2(type1,types1,types2,typatory,border)
#print(type1, type2, typatory)
#    
#
#type2=[[1],2]
#print(type2[0])                         
#types1=dict()
#types2=dict()
#types2[json.dumps([1, 999, 0])]=0
#types2[json.dumps([7, 999, 6])]=0
#types2[json.dumps([0, 999, 6])]=0
#types1[json.dumps([1,888,1,999,6])]=-1
#typatory=dict()
#border=1
#initial_value_types=0
def new_type1(type2,types2,types1,typatory,border,assigned_types):                                   #much like new_type2 but mirrored
    #print("new type 1 debug. type2",type2,"types1",types1,"types2",types2,"typatory",typatory,"border",border )
    #låt oss nu anta att en typ ser ut såhär: [1,888,2] eller [1,999,2]. 888=over /. 999=under \. den blir jsondumpad för att kunna vara en key i en dict. här loadar vi den tillbaka till en lista för att enkelt kunna manipulera den.             
#    print(type2)
    ts1=True                                                                   #We start by assuming that s1 will be typed
    type1=0                                                                    #assign value to type 1 even if it will not be used, so that the function can return something
    if len(type2[0])==1:                                                       #if first position in type 2 is a primitive, the simplest compatible type is s/type2 i.e. 0,888,type2[0]
        if type2[0]==[0]:                                                      #but if type2 is a sentence, type1 is not typed. this is becaues a sentence does not expect something and should be able to stand alone.
            ts1=False
        else:
            primitive=0                                                            
            type1=[[primitive,888,type2[0][0]],initial_value_types]            #when a new type is assigned based on the type of the other element, the simplest way is chosen. if the other is a primitive, element will reduce to s (0) with it. if the other element expects sometning, lowest level expectation will be fulfilled.
            type1string=json.dumps(type1[0])                                   #type1 as a key is the first element in list
            
            while type1string in types1 and types1[type1string]<limit_value_for_bad_type:                                       #this means they are existing bad types
                primitive+=1
                type1[0][0]=primitive                                          #varje gång det inte funkar byter man primitiv i den nya typen, dvs vad den nya typen reducerar till. NYA PRIMITIVER KAN då uppstå!! OBS viktigt!!
                type1string=json.dumps(type1[0])                                   #
#            if len(assigned_types)==3: #om assigned types är mer än en lång (egentligen mer än noll) så är vi inte postborder och vi kan ha a/b,b elkler a\b,b innan. i första fallet måste a ändras till primitive. i andra fallet måste b ändras till primitive
#                print("new design type 1 modification, assigned types pre" , assigned_types, "type2 is primitive:",type2)
#                assigned_types,sentence_condition,ts1=assign_two_types_that_reduce_to_x(primitive,assigned_types,ts1) #Nmakes sure the two previouslu assigned types arechanged to reduce to primitive
#                assigned_types[1][1].append(888)   #and the last type of appended types also needs to be modified, because new type1 is primitive over type two, so that is what the two elements should reduce to
#                assigned_types[1][1].append(type2[0][0])  
#                print("new design type 1 modification, assigned types post" , assigned_types)
#                c
#            if type1string not in typatory:
#                typatory[type1string]=initial_value_types                          #add this type to typatory if it is not there alread(SKIP, this step is taken in the end right before reinforcement)    
    else:                                                                      #if type2 is not a primitive we are only interested in what it expects backwards. thus what precedes any 999 can be assigned as a type to s1
        for i in range((len(type2[0]))):                                       #normal loop. this is because we want to test the lowest level expectation first. this favours simplicity and simulates simpler interpretations from the learner
            if type2[0][i]==999:
                type1=[type2[0][:i],initial_value_types]                       #slices list and makes what comes after / (bc backwards loop defined as position len(type1[0])-i type2, independently of if it's a primitive or primitive\x
                #print(type1)
                type1string=json.dumps(type1[0])
                if type1string not in types1:                                  #means  that the suggested type is not an existing bad type and can be assigned to s1.
#                    if type1string not in typatory:
#                        typatory[type1string]=initial_value_types              #insert new type in typatory before breaking loop
                    break                    
            else:                                                          #means that all type2 expectations are bad types for s1. we need to change the lowest level expectation in type 2 by replacing it with a primitive that is not the one currently used
                new_primitive=0
                for i in range((len(type2[0]))):                           #loop again
                    if type2[0][i]==999:
                        type2[0][i-1]=new_primitive                        #replace lowest level primitive in type 2 with new primitive so we can test if it makes a bad type for s2. it doesn't matter if lowest expected type is primitive or of the type a/b. lowest level primitive will always precede first \. the simplest way to invent a new type is to change the lowest level primitive. in the most common case, s2 will expect a primitive, we know that s1 is not this primitive, so we will give s1 a new primitive and add and assign the type that expects this primitive to s2. both are modified.
                        type1=[type2[0][:i],initial_value_types]
                        type1string=json.dumps(type1[0])                   #dump it so it can be searched for in types1
                        type2string=json.dumps(type2[0])
                        while (type1string in types1 and types1[type1string]<limit_value_for_bad_type) or (type2string in types2 and types2[type2string]<limit_value_for_bad_type):  #count from 0 and upwards until find a number that is not a bad type for s2 or will make a new bad type for s1
                           new_primitive+=1
                           type2[0][i-1]=new_primitive                     #replace lowest level primitive in type 2 with new primitive so we can test if it makes a bad type for s2. it doesn't matter if lowest expected type is primitive or of the type a/b. lowest level primitive will always precede first /. the simplest way to invent a new type is to change the lowest level primitive. in the most common case, s2 will expect a primitive, we know that s1 is not this primitive, so we will give s1 a new primitive and add and assign the type that expects this primitive to s2. both are modified.
                           type1=[type2[0][:i],initial_value_types]
                           type1string=json.dumps(type1[0])                #dump it so it can be searched for in types1
                           type2string=json.dumps(type2[0])
                           #print("LOOPING IN NEW TYPE 1 ")
                           #print("new type 1 debug. type1string",type1string,"types1",types1,"type2string",type2string,"types2",types2)
                        if type2string not in types2:
                            type2[1]=initial_value_types                       #if type 2 is modified it will restart at initial value. type2[0] is already defined above.
                        else:
                            type2[1]=types2[type2string]
#        if len(assigned_types)==3: #om assigned types är mer än en lång (egentligen mer än noll) så är vi inte postborder och vi kan ha a/b,b elkler a\b,b innan. i första fallet måste a ändras till primitive. i andra fallet måste b ändras till primitive
#            print("new design type 1 modification, assigned types pre" , assigned_types, "type2 is primitive under something:",type2)
#            assigned_types,sentence_condition,ts1=assign_two_types_that_reduce_to_x(new_primitive,assigned_types,ts1) #Nmakes sure the two previouslu assigned types arechanged to reduce to primitive
#            print("new design type 1 modification, assigned types post" , assigned_types)         
#            x
#                if type1string not in typatory:
#                    typatory[type1string]=initial_value_types              #add both new types to typatory if they are not there already. s_r_value_types is updated in assign_types.
#                if type2string not in typatory:
#                    typatory[type2string]=initial_value_types
    if type1==0: ts1=False                                                      #if the function has not found a way to assign type1, for example if type2
    return type1, type2, typatory, ts1       
    
#TEST NEDAN
#type1, type2, typatory =new_type1(type2,types2,types1,typatory)
#print(type1, type2, typatory)
    

#NEDAN SUDDA VA? FUNKTIONEN FIXAD OVAN choose_compatible_pair
##def choose_compatible_types(type1,types1,types2,typatory):                     #type1 därför att om den ärvs från tidigare chunkning är processen en annan. In this pilot, for simplicity, if type1 is inherited, the rest of the typing adapts to this. this can be changed in future versions.
#        type1,type1_index=choose_type(types1)
#        type2=choose_compatible_type_s2_constrained_by_s1(type1,types2)
#        if type2==0:
#            types1[0].remove(types1[0][type1_index])
#            types1[1].remove(types1[1][type1_index])
#            if len(types1[0]
#            best_remaining_type=
#            type2,type2_index=ct_s2_unconstrained()
#            type1=ct_s1_constrained_by_s2(s2,types1)
#            if type1=0:
#                types

# NEDAN: ALLA TYPER AV ÖVERLAPPNINGAR RÄKNAS SOM KOMPATIBLA. TÄNK ÄVEN ÖVER GRÄNSSITUATIONER. DELS OM TYPE1 ÄR SENTENCE. MEN VAD HÄNDER OM TPE1 FÖRVÄNTAR SIG EN KOMPLEX PRIMITIV PÅ NÅGON NIVÅ? det skiter vi i just nu.        

def ct_s1_unconstrained(types):                                                #ct means compatible type. alla ct-funktioner tar en dict med typer (utvalda good types m positiva värden) som argument och returnar en dict med de typer som är kompatibla i den angivna kontexten
    compatible_types=dict()
    for i in types:
        typelist=json.loads(i)
        #print("type to be checked for compatibility with position 1:",i)
        if len(typelist)==1 or typelist[1]!=999:                             #it may be less trivial, but for now: den enda restriktionen för s1 är att den inte får ha \ om det inte föregås av / dvs ingen 999 som inte föregås av 888
            compatible_types[i]=types[i]
    return compatible_types
    
def ct_s1_constrained_by_s2(type2,types):                                      #here there are three options. 1: type1 is or ends with anything that precedes any 999 in type2. 2: type 2 is or starts with what follows any 888 in type1. 3: s1 är 0 och s2 är kompatible med position 1 unconstrained                
    #print("debug ct_s1_constrained_by_s2: type2", type2, "types",types)
    compatible_types=dict()
    type_compatible=False
    for i in types:
        type1list=json.loads(i)
        if type1list==[0]:                                                     #type1 is a sentence
            if type2[0][:1]==[0,999]:          #type2 takes a preceding s as an argument.
                compatible_types[i]=types[i]
                type_compatible=True
        else:                                                                  #type1 is not a sentence                               
            #for j in range(len(type2[0])):                                                    #check if type2 has any unders and if what precedes them matches type1
            if len(type2[0])>2:
                if type2[0][1]==999 and type1list==[type2[0][0]]:
                #if len(type1list)>=len(type2[0][:j]):                         #type1 needs to be at least as long as the chunk we are searchin for an overlap with
                        #if type1list[len(type1list)-len(type2[0][j:]):]==type2[0][:j]: #defines overlap
                    compatible_types[i]=types[i]
                    type_compatible=True
        if not type_compatible:                                                #if the type has not been matched in the two first alternatives, check if there are overs in type1 and if what follows them matches type2
            #for k in range(len(type1list)):
            if len(type1list)>2:
                #print("WTF",type1list[len(type1list)-2], type2[0][0],type1list[len(type1list)-1])
                if type1list[len(type1list)-2]==888 and type2[0][0]==type1list[len(type1list)-1]:
                    if len(type2[0])>2:
                        if type2[0][1]!=999:
                    #if len(type2[0])>=len(type1list[k+1:]):                            #type2 needs to be at least as long as the chunk we are searching for an overlap with
                        #if type1list[k+1:]==type2[0][:len(type1list[k+1:])]:       #defines overlap
                            compatible_types[i]=types[i]
    return compatible_types
        
    
def ct_s2_constrained_by_s1(type1,types):                                      #like ct_s1_constrained_by_s2 but mirrored       
    #print("debug ct_s2_constrained_by_s1: type1", type1, "types",types)
    compatible_types=dict()
    type_compatible=False
    for i in types:
        type2list=json.loads(i)
        if type1[0]==[0]:                                                         #type1 is a sentence
            if type2list[:1]==[0,999]:  #type2 takes a preceding s as an argument
                compatible_types[i]=types[i]
                type_compatible=True
        else:                                                                  #type1 is not a sentence                               
            #for j in range(len(type2list)):                                                #check if type2 has any unders and if what precedes them matches type1
            if len(type2list)>2:
                if type2list[1]==999 and type1[0]==type2list[0]:
                    #if len(type1[0])>=len(type2list[:j]):                         #type1 needs to be at least as long as the chunk we are searchin for an overlap with
                        #if type1[0][len(type1[0])-len(type2list[j:]):]==type2list[:j]: #defines overlap
                    compatible_types[i]=types[i]
                    type_compatible=True
        if not type_compatible:                                                #if the type has not been matched in the two first alternatives, check if there are overs in type1 and if what follows them matches type2
            #for k in range(len(type1[0])):
            if len(type1[0])>2:   
                if type1[0][len(type1[0])-2]==888 and type2list[0]==type1[0][len(type1[0])-1]:
                    if len(type2list)>2:
                        if type2list[1]!=999:
                    #if len(type2list)>=len(type1[0][k+1:]):                            #type2 needs to be at least as long as the chunk we are searching for an overlap with
                        #if type1[0][k+1:]==type2list[:len(type1[0][k+1:])]:           #defines overlap
                            compatible_types[i]=types[i]
    return compatible_types
        
    
    
#def compatible_ct #? vet inte om detta är en funktion som ska göras??

#            w=[]
#            b_values är ju då en lista med värdet för alla beteenden   
#            for i in range(len(b_values)):                                    #klistrade in bara för att ha framför ögonen hur beslut fattas i vanliga fall
#                w.append(exp(beta*b_values[i]))
#            q=random()*sum(w)                                                 #random value from sum of support
#            response=0
#            while q>sum(w[0:response+1]): response+=1 

        
def choose_type(types):                                                        #types måste vara en dict. ska returnera en lista som ska vara den valda typen och dess värde
    typeslist=[]
    valueslist=[]
    bvalueslist=[]
    chosentype=[]    
    for i in types:
        typeslist.append(json.loads(i))                                         #append key, i.e the type to typeslist
        bvalueslist.append(exp(beta*types[i]))                                   #append exponent of beta * the type's value to bvalueslist
        valueslist.append(types[i])
    q=random()*sum(bvalueslist)                                                 #valueslist är en lista med v-värden för typerna
    response=0
    while q>sum(bvalueslist[0:response+1]): response+=1    
    chosentype=[typeslist[response],valueslist[response]]
    return chosentype                                                          #returns the chosen type and its value as a list, and the response index pga behövs i dominant type
        

def choose_dominant_type(types1,types2,typatory):
    #print("input choose dominant type: good types 1", types1, "good types 2", types2)
    types1and2=[]                                                              #detta är en liosta som ska fyllas med listor som är alla typer.
    values=[]                                                                  #fills up with the types' values modified with exp and beta.
    bvalues=[] 
    index=[]                                                                   #fills up with ones and twos so we can now after the coice if the cosen type belongs to s1 or s2
    for i in types1:
        types1and2.append((json.loads(i)))
        values.append(types1[i])
#        if i in typatory:
#            bvalues.append(exp(beta*typatory[i]))
#        else:
        bvalues.append(exp(beta*types1[i]))                                    #separeate values from the b value that is used in decision, do that original value can be appended to the type, that should be a list contaioning type and value
         
        index.append(1)
    for i in types2:
        types1and2.append((json.loads(i)))
        values.append(types2[i])
#        if i in typatory:
#            bvalues.append(exp(beta*typatory[i]))
#        else:
        bvalues.append(exp(beta*types2[i]))
        index.append(2)
    #print("types1and2",types1and2)
    q=random()*sum(bvalues)
    response=0
    while q>sum(bvalues[0:response+1]): response+=1
    #print("dominant type response",response)
    dominant_type=[types1and2[response],values[response],index[response]]
    #print("dominant type",dominant_type)
    if dominant_type[2]==1:
        compatible_types=ct_s2_constrained_by_s1(dominant_type,types2)
        #print("compatible types 2", compatible_types)
    else:
        compatible_types=ct_s1_constrained_by_s2(dominant_type,types1)
        #print("compatible types 1", compatible_types)
    if len(compatible_types)==0:                                               #there were no compatible types for the chosen dominant type. success is false and there will be another round without the chosen type
        success=False
        compatible_type=0                                                      #just for the function to have something to return for compatible type
#        print("choose dominant type debug: types1",types1,"types 2", types2, "dominant type",dominant_type)
        if dominant_type[2]==1:                                                #Means the chosen type is from types1
            types1.pop(json.dumps(dominant_type[0]))                           #removes the failed dominant type from dict.
        else:                                                                 #the chosen type is from types2
            types2.pop(json.dumps(dominant_type[0]))                           #removes the failed dominant type from dict
    else:
        success=True
        compatible_type=choose_type(compatible_types)
    return dominant_type,compatible_type,success,types1,types2                 #ska returnera en vald typ, en kompatibel typ om möjligt, booleansk succe samt, om succen failar, en dict där den valda dominanta typen är borttagen.    
     
#DATA      
def generate_stimuli_reinforcement(sentences):                                 #sentence_structure: (adjective) noun verb ((adjective) noun (verb (adjective) noun)) if in parentheses can occur or not
    stimuli=[]
    reinforcement=[]    
    for i in range(sentences):
        if p_adjective>random():
            if zipf_distribution==True:
                word=(zipf_distribute(n_adjectives,300))
                stimuli.append(word)
            else:
                stimuli.append(randint(300,299+n_adjectives))                      #adjectives are 300-something
            reinforcement.append(negative_reinforcement)                       #-r
        stimuli.append(randint(1,n_subjects))                                  #nouns are 1-something
        reinforcement.append(negative_reinforcement)                           #-r
        if p_verbalized_noun>random():                                         
            stimuli.append(randint(1,n_verbalized_nouns))                      #verbalized nouns are verbs but have the form of nouns. 100-something
            reinforcement.append(positive_reinforcement)                       #+r for simplicity: verbalized nouns are here
        elif p_intransitive_verb>random():                                     #
            if zipf_distribution==True:
                word=(zipf_distribute(n_intransitive_verbs,400))
                stimuli.append(word)
            else:
                stimuli.append(randint(400,399+n_intransitive_verbs))              #intransitive verbs are 400-something
            reinforcement.append(positive_reinforcement)                       #+r
        else:
            if zipf_distribution==True:
                word=(zipf_distribute(n_transitive_verbs,100))
                stimuli.append(word)
            else:
                stimuli.append(randint(100,99+n_transitive_verbs))                 #transitive verb are 100-something
            reinforcement.append(negative_reinforcement)                       #-r
            if p_adjective>random():                                           #only begin a new NP after transitive verb
                if zipf_distribution==True:
                    word=(zipf_distribute(n_adjectives,300))
                    stimuli.append(word)
                else:
                    stimuli.append(randint(300,299+n_adjectives))
                reinforcement.append(negative_reinforcement)
            stimuli.append(randint(1,n_objects))                               #object, only after transitive verb
            for i in range(max_n_subordinate_clauses):                         #subordinate clause, only after object
                if p_subordinate_clause>random():
                    reinforcement.append(negative_reinforcement)
                    if p_subordinate_clause_init>random():
                        if zipf_distribution==True:
                            word=(zipf_distribute(n_subordinate_clause_init,200))
                            stimuli.append(word)
                        else:
                            stimuli.append(randint(200,199+n_subordinate_clause_init))
                        reinforcement.append(negative_reinforcement)
                    if p_verbalized_noun>random():
                        if zipf_distribution==True:
                            (zipf_distribute(n_verbalized_nouns,1))
                            stimuli.append(word)
                        else:
                            stimuli.append(randint(1,n_verbalized_nouns))
                        reinforcement.append(positive_reinforcement)
                    elif p_intransitive_verb>random():
                        if zipf_distribution==True:
                            word=(zipf_distribute(n_intransitive_verbs,400))
                            stimuli.append(word)
                        else:
                            stimuli.append(randint(400,399+n_intransitive_verbs))              #intransitive verbs are 400-something
                        reinforcement.append(positive_reinforcement)                       #+r
                    else:
                        if zipf_distribution==True:
                            
                            word=(zipf_distribute(n_transitive_verbs,100))
                            stimuli.append(word)
                        else:
                            stimuli.append(randint(100,99+n_transitive_verbs))
                        reinforcement.append(negative_reinforcement)
                        if p_adjective>random():
                            if zipf_distribution==True:
                                word=(zipf_distribute(n_adjectives,300))
                                stimuli.append(word)
                            else:
                                stimuli.append(randint(300,299+n_adjectives))
                            reinforcement.append(negative_reinforcement)
                        if zipf_distribution==True:
                            word=(zipf_distribute(n_objects,1))
                            stimuli.append(word)
                        else:
                            stimuli.append(randint(1,n_objects))                            
            reinforcement.append(positive_reinforcement)                           #correct response: don't place boundary after stimulus                                              #correct response: place boundary after stimulus
    return stimuli,reinforcement
#stimuli,reinforcement=generate_stimuli_reinforcement(10)
#print(stimuli,reinforcement)

def zipf_distribute(n_words,wordclass_start):
    zipflist=[]
    for i in range(n_words):
        zipflist.append(2**i)
    q=random()*sum(zipflist)                                                 #zipflist är en lista med tvåpotenser, dvs en zipfdistribution
    response=0
    while q>sum(zipflist[0:response+1]): response+=1     
    word=wordclass_start+response
    return word

#------------------------------------------------------------
#                            Simulation
#------------------------------------------------------------


def simulate(stimuli,reinforcement,n_trials,simulation_runs):
    reinforcements=[0 for t in range(n_trials)]
    for i in range(simulation_runs):
        sr_value_pairs = dict()                                                #long term memory that indicates values of the responses for couples of stimuli
        sr_value_types = dict()                                                #long term memory that indicates all types ever assigned to each element and the value of assigning each type.for each element, the dictionary contains a dictionary containing types and values.
        typatory = dict()                                                      #long term memory that contains all types ever assigned and their general value - determines likelyhood of type being assigned to new element when there are several possibilities.
        s1=stimuli[0]
        s1_index=0
        s2_index=1
        controller=0
        pairs=[]
        responses=[]
#        primitive_counter=0 #tror att denna utgått pga man testar alltid från noll och uppåt tills det blir en typ som inte är en dålig typ
        inherited_type1=0
        assigned_types=[]
        border=True
        for t in range(n_trials):
            s1,s1_index,s2_index,pairs,responses,sr_value_pairs,success,controller,inherited_type1,assigned_types,border=respond(stimuli,reinforcement,s1,s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types,border)         #runs respond and learn and redefines arguments for next trial
            #print(u)
            reinforcements[t] += success/simulation_runs
    #print(sr_value_pairs)
    print("SRVALUETYPES",sr_value_types)
    good_sr_value_types=dict()
    for i in sr_value_types:
        good_sr_value_types[i]=dict()
        for j in sr_value_types[i]:
            if sr_value_types[i][j]>0:

                good_sr_value_types[i][j]=sr_value_types[i][j]
    print("GOODSRVALUETYPES",good_sr_value_types)
    #print(assigned_types)
    return reinforcements                                                      #returns a list with average for correct response after each trial


#------------------------------------------------------------
#                            Parser
#------------------------------------------------------------

def build_parser():
    stimuli,reinforcement = generate_stimuli_reinforcement(sentences)
    sr_value_pairs = dict()                                                        #long term memory that indicates values of the three responses for couples of stimuli
    sr_value_types=dict()
    typatory=dict()
    s1=stimuli[0]
    s1_index=0
    s2_index=1
    pairs=[]
    responses=[]
    controller=1
    inherited_type1=0
    assigned_types=[]
    border=True
    for t in range(n_trials):
        s1,s1_index,s2_index,pairs,responses,sr_value_pairs,sucess,controller,inherited_type1,assigned_types,border=respond(stimuli,reinforcement,s1,s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types,border)         #runs respond and learn and redefines arguments for next trial
    parsed_sentences=dict()
    for pair in sr_value_pairs:
        if sr_value_pairs[pair][0]>threshold:
            parsing=json.loads(pair)[0]
            string=[]
            if (type(parsing)==list):
                for i in parsing:
                    if (type(i)==list):
                        for j in i:
                            string.append(j)
                    else: string.append(i)
            else: string.append(parsing)
            hash=json.dumps(string)
            if hash in parsed_sentences and parsing not in parsed_sentences[hash]:
                parsed_sentences[hash].append(parsing)
            else:
                parsed_sentences[hash]=[]
                parsed_sentences[hash].append(parsing)
    print(parsed_sentences)
    return parsed_sentences


    
def test_parser(n_tests,maxlength_test,n_test_elements):
    parsed_sentences=build_parser()
    for i in range(n_tests):
        teststring=[]
        for i in range(randint(1,maxlength_test)):
            teststring.append(randint(1,n_test_elements))
        hash=json.dumps(teststring)
        print(hash)
        if hash in parsed_sentences:
            print('Correct. Parsing: ',parsed_sentences[hash])
        else: print('No sentence')
            
    

#------------------------------------------------------------
#                         Fig
#------------------------------------------------------------
##DATA
#n_subjects=10
#n_objects=10
#n_transitive_verbs=0
#n_intransitive_verbs=10
#n_verbalized_nouns=0 #are always intransitive in this version
#n_subordinate_clause_init=0
#n_adjectives=0
#max_n_subordinate_clauses=0
#p_subordinate_clause_init=0
#p_subordinate_clause=0
#p_verbalized_noun=0
#p_adjective=0
#p_intransitive_verb=1

    
def fig_learning():
    stimuli,reinforcement = generate_stimuli_reinforcement(sentences)    
    plt.subplot(1,1,1)
    reinforcements = simulate(stimuli,reinforcement,n_trials,simulation_runs)
    plt.plot(reinforcements, 'k',linestyle='-', color = 'red', label = 'flexible, types,  no retype of chunk')
    title = str(n_subjects) + ' nouns ' + str(n_intransitive_verbs) + ' intransitive verbs ' + str(n_transitive_verbs) + ' transitive verbs '
    if not zipf_distribution:
        title += ' no Zipf'
    plt.title(title)
    plt.xlabel('Trial')
    plt.ylabel('Frequency of correct responses')
    plt.legend(title='Learning mechanism',loc = 'best')
    
    plt.show()
    
#    n_subjects=2
#n_objects=2
#n_transitive_verbs=0
#n_intransitive_verbs=2
#n_verbalized_nouns=0 #are always intransitive in this version
#n_subordinate_clause_init=0
#n_adjectives=0
#max_n_subordinate_clauses=0
#p_subordinate_clause_init=0
#p_subordinate_clause=0
#p_verbalized_noun=0
#p_adjective=0
#p_intransitive_verb=1
#zipf_distribution=False
    
    
def see_grammar():
    grammar = build_parser()
    for key in grammar:
        print('Correct sentence:' + str(key))
        l = len(grammar[key])
        print('Number of possible parsing: ' + str(l))
        for i in range(l):
            print('Parsing ' + str(i+1) +': ' + str(grammar[key][i]))
            
def build_parser_new():
    stimuli,reinforcement = generate_stimuli_reinforcement(sentences)
    sr_value_pairs = dict()                                                        #long term memory that indicates values of the tree responses for couples of stimuli
    s1=stimuli[0]
    s1_index=0
    s2_index=1
    pairs=[]
    responses=[]
    for t in range(n_trials):
        s1,s1_index,s2_index,pairs,responses,sr_value_pairs,u=respond(stimuli,reinforcement,s1,s1_index,s2_index,pairs,responses,sr_value_pairs,sr_value_types,typatory,controller,inherited_type1,assigned_types)         #runs respond and learn and redefines arguments for next trial
    
    #print(sr_value_pairs)
    # Extraction of successful subchunks
    good_chunks = collect_good_chunks(sr_value_pairs, 0.5)
    #print(good_chunks)
    
    # Extraction of successful sentences
    parsed_sentences=dict()
    for pair in sr_value_pairs:
        if sr_value_pairs[pair][2]>0.95:
            parsing=json.loads(pair)[0]
            if parsing in good_chunks:
                #print('Good sentence')
                #print(parsing)
            #print(parsing)
                string=[]
                for i in parsing:
                    if (type(i)==list):
                        for j in i:
                            string.append(j)
                    else: string.append(i)
                hash=json.dumps(string)
                if hash in parsed_sentences and parsing not in parsed_sentences[hash]:
                    parsed_sentences[hash].append(parsing)
                else:
                    parsed_sentences[hash]=[]
                    parsed_sentences[hash].append(parsing)
    
    return good_chunks, parsed_sentences

def subchunk(pair):
    return [pair[0][0],[pair[0][1],pair[1] ] ]

def collect_good_chunks(sr_value_pairs, threshold = 0.5):
    good_chunks = []
    for pair in sr_value_pairs:
        #print(pair)
        if sr_value_pairs[pair][0] > threshold:
            #print('Good chunk')
            good_chunks.append(json.loads(pair))
        if sr_value_pairs[pair][1] > threshold:
            spair = subchunk(json.loads(pair))
            #print(spair)
            good_chunks.append(spair)
            if spair[1] not in good_chunks:
                good_chunks.append(spair[1])
        
    return good_chunks
    
    
    
#------------------------------------------------------------
#PLOT LEARNING CURVE FOR PARSER:
fig_learning()

#TEST PARSER ON RANDOM STRINGS:
#test_parser(n_tests,maxlength_test,n_test_elements)

#PRINT GRAMMAR
#see_grammar()






